{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "для поиска будем пользоваться word2vec, и чтобы размеры файлов не были такими большими, оставим только слова, которые есть в исходных данных.\n",
    "\n",
    "Исходные файлы:\n",
    "\n",
    "* `wiki-news-300d-1M.vec` из https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
    "\n",
    "* `english_cleaned_lyrics.csv` из https://github.com/hiteshyalamanchili/SongGenreClassification/tree/master/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "lyrics = pd.read_csv(r'english_cleaned_lyrics.csv')\n",
    "lyrics = lyrics[['song','artist','lyrics']]\n",
    "lyrics.loc[lyrics.song.isna(), 'song'] = 'green-onions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем все слова, используемые в песнях  и названиях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize(x):\n",
    "    s = x.split('-')\n",
    "    return ' '.join([w[0].upper() + w[1:] for w in s])\n",
    "\n",
    "lyrics.artist = lyrics.artist.apply(capitalize)\n",
    "lyrics.song = lyrics.song.apply(capitalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def is_letter_or_space(x):\n",
    "    return ord('a') <= ord(x) <= ord('z') or x == ' '\n",
    "\n",
    "def filter_string(s):\n",
    "    return ''.join(filter(is_letter_or_space, map(str.lower, s)))\n",
    "\n",
    "filtered_lyrics = lyrics.lyrics.apply(filter_string) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "\n",
    "for w in lyrics.song:\n",
    "    all_words.extend(w.lower().split())\n",
    "\n",
    "for w in filtered_lyrics:\n",
    "    all_words.extend(w.split())\n",
    "    \n",
    "all_words = set(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics.rename(columns={'song':'title'}, inplace = True)\n",
    "lyrics.to_csv('data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим файл на 5, чтобы хватило оперативной памяти\n",
    "\n",
    "`$ split -l 200000 wiki-news-300d-1M.vec -d --additional-suffix=.csv f`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И оставим в таблицах только те слова, которые встеритлись в песнях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "for k in range(5):\n",
    "    data = [[] for _ in range(301)]\n",
    "    words = set()\n",
    "    with codecs.open(f\"f0{k}.csv\", \"r\", \"utf_8_sig\" ) as f:\n",
    "            for line in f:\n",
    "                l = line.split()\n",
    "                if len(l)==301:\n",
    "                    for i, o in enumerate(l):\n",
    "                        if i == 0:\n",
    "                            word = o.lower()\n",
    "                            if word in words:\n",
    "                                break\n",
    "                            else:\n",
    "                                words.add(word)\n",
    "                            data[i].append(word)\n",
    "                        else:\n",
    "                            data[i].append(o)\n",
    "\n",
    "    labels = list(map(str, ['word'] + list(range(300))))\n",
    "    x = pd.DataFrame({labels[i] : data[i] for i in range(301)})\n",
    "    index = [w.lower() in all_words for w in x.word]\n",
    "\n",
    "    x[index].to_csv(f't0{k}.csv', header= k==0, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь соберем все в один файл\n",
    "\n",
    "`$ cat t00.csv t01.csv t02.csv t03.csv t04.csv > words.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученые файлы\n",
    "* `data.csv` таблица с песнями\n",
    "* `words.csv` word2vec таблица слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
